{"cells":[{"cell_type":"markdown","metadata":{"id":"DV45IZ9474vp"},"source":["#**Práctica 5: Introducción al Procesamiento Digital de Imágenes y Visión por Computador**\n","\n","Curso: Inteligencia Artificial para Ingenieros\n","\n","Prof. Carlos Toro N. (carlos.toro.ing@gmail.com)\n","\n","2022"]},{"cell_type":"markdown","metadata":{"id":"gcYaBcWu8NH4"},"source":["**Introducción**"]},{"cell_type":"markdown","metadata":{"id":"MYwBZnFIDoQt"},"source":["En esta práctica introduciremos algunos de los conceptos básicos del procesamiento digital de imágenes y visión por computador usando la [librería OpenCV](https://opencv.org/releases/)."]},{"cell_type":"markdown","metadata":{"id":"WyqQiDJb8RI3"},"source":["**Dentro de los temas que veremos están:**\n","\n","1. Procesamiento básico\n","2. Histogramas y enmascaramiento\n","3. Extraer características básicas\n","4. Detector de objetos en cascada: detección de rostros"]},{"cell_type":"markdown","metadata":{"id":"lMJMHb-BsmHM"},"source":["**PREVIO:** Importaciones necesarios para la práctica"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import cv2, matplotlib\n","import numpy as np"],"metadata":{"id":"AnMcX8N8146v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Is952HwcNu_K"},"source":["## 1. Lectura y caracterización básica de imágenes"]},{"cell_type":"markdown","metadata":{"id":"MX0gvQSWUZ20"},"source":["Leyendo una imagen en OpenCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEymOxxEXqBK"},"outputs":[],"source":["# leamos una imagen\n","img = cv2.imread('manzano.jpg')\n","\n","# mostremos el formato de la imagen (básicamente un arreglo 3D de pixeles con información de color, en formato BGR)\n","print(f\"La imagen tiene dimensiones {img.shape}, en estructura de dato {type(img)}\\n\")#la primera dimensión corresponde al ancho y la segunda al alto\n","\n","# n° de canales y bits que codifican los niveles de intensidad\n","print(f\"La imagen tiene {img.shape[-1]} canales, cada uno codificado con datos de tipo {img.dtype}\\n\")#notar que una imagen en escala de grises posee solo 1 canal.\n","\n","# Mostremos algunos valores de la imagen\n","print(f\"Algunos valores de los niveles de intensidad de la imagen por canal de color BGR:\\n\\n{img[0:3,0:3,:]}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"UdbgSxNfbr_l"},"source":["Si queremos visualizar la imagen con `matplotlib`, veremos que los colores aparecerán de forma extraña, esto se debe a que opencv lee las imágenes con los canales de color en orden BGR (Blue, Green, Red) y matplotlib espera que vengan en formato RGB. Despleguemos la imagen en ambos formatos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ih75-vTYcLwM"},"outputs":[],"source":["# transformemos la imagen a formato de color RGB\n","img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","\n","# despleguemos ambas imágenes\n","fig = plt.figure(figsize=(6,2.5),dpi = 150)\n","plt.subplot(1,2,1)\n","plt.imshow(img)\n","plt.title(\"Imagen en formato BGR\")\n","\n","plt.subplot(1,2,2)\n","plt.imshow(img_rgb)\n","#plt.axis('off')#para quitar la numeración de los ejes\n","plt.title(\"Imagen en formato RGB\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kC6mgeIaea4t"},"source":["Una operación que puede resultar útil en algunos pre-procesamientos, es la transformación de la imagen original a escala de grises, con la función `cvtColor` podemos hacerlo. Matemáticamente, OpenCV realiza la siguiente operación: $Y_{out}=0.299*R + 0.587*G + 0.114*B$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMoPCI8Fq2Pw"},"outputs":[],"source":["# convertimos la imagen a escala de grises\n","gray_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n","\n","# ahora, la imagen en escala de grises es solo un arreglo 2D\n","print(f\"La imagen tiene dimensiones {gray_img.shape}, en un formato de dato {type(gray_img)}\\n\\n\")\n","\n","# despleguemos ambas imágenes\n","fig = plt.figure(figsize=(6,2.5),dpi = 150)\n","plt.subplot(1,2,1)\n","plt.imshow(img_rgb)\n","plt.title(\"Imagen en formato RGB\")\n","\n","plt.subplot(1,2,2)\n","plt.imshow(gray_img,cmap = \"gray\")\n","plt.title(\"Imagen en escala de grises\")\n","plt.show()"]},{"cell_type":"markdown","source":["**Ejercicio**: Graficar los canales de color por separado de la imagen"],"metadata":{"id":"eR88qg1HiAyP"}},{"cell_type":"code","source":["# Código aquí"],"metadata":{"id":"LpXNHLurWl6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Transformación a espacio HSV**"],"metadata":{"id":"nDsuAbG6Wlrc"}},{"cell_type":"code","source":["img_hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n","#visualicemos los canales por separado\n","fig     = plt.figure(figsize=(16,2.5),dpi = 150)\n","titulos = [\"canal Hue\", \"Canal Saturation\", \"Canal Value\"]\n","\n","for i in range(3):\n","  plt.subplot(1,3,i+1), plt.imshow(img_hsv[:,:,i],cmap = \"gray\")\n","  plt.title(titulos[i])\n","  plt.xticks([]), plt.yticks([])\n","\n","plt.show()"],"metadata":{"id":"j_4RhDt3WrGK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Operaciones globales sobre la imagen"],"metadata":{"id":"U4aPSLKwyopr"}},{"cell_type":"markdown","source":["Ya que vimos que las imágenes son arreglos, podemos aplicar fórmulas o extraer descriptores estadísticos globales (un solo valor o característica que represente a la imagen completa) sobre ellas."],"metadata":{"id":"OPcY2C8TyZ_C"}},{"cell_type":"markdown","source":["**A. Promedio de una imagen a color**"],"metadata":{"id":"8jJg2xhRzCe0"}},{"cell_type":"code","source":["# encontremos el promedio de la imagen a color anterior, el 'color promedio'\n","average_color = np.average(img_rgb,axis=(0,1))#promedio por cada canal de color\n","\n","# lo anterior equivale a calcular el promedio por separado por cada canal de color y guardar el resultado en un array\n","average_color2= np.uint8(np.array([np.average(img_rgb[:,:,0]),np.average(img_rgb[:,:,1]),np.average(img_rgb[:,:,2])]))\n","\n","# llevemos el valor decimal anterior al mismo tipo de datos de la imagen\n","# el color promedio será\n","average_color = np.uint8(average_color)\n","print(f\"Color promedio calculado de forma compacta:  {average_color}\")\n","print(f\"Color promedio calculado de forma extendida: {average_color2}\")"],"metadata":{"id":"2eOv0PuozGQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Formemos un pequeño arreglo con los valores calculados para desplegar el color promedio como si fuera una imagen."],"metadata":{"id":"asUjaVQq2b2Q"}},{"cell_type":"code","source":["average_color_img = np.array([[average_color]*100]*100,np.uint8)\n","plt.imshow(average_color_img)\n","plt.show()"],"metadata":{"id":"3rUBBEHM2o4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**B. Umbralizado y binarización de imágenes**"],"metadata":{"id":"e0Ombp9_4Dmy"}},{"cell_type":"markdown","source":["Experimentemos creando un umbral y binarizando (solo 2 valores) la imagen original. Con esto tendremos que para un valor umbral $u$, un pixel de imagen $p$, si $p>u$ entonces se tendrá un valor 1 y 0 para el caso contrario, o codificada en 8 bits, de 255 o 0 respectivamente. Partamos con la imagen en escala de grises, ya que es más simple entender el concepto con un solo arreglo:"],"metadata":{"id":"VikrdZQk4O0d"}},{"cell_type":"code","source":["help(cv2.threshold)"],"metadata":{"id":"LQkXvSJk-Rge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["UMBRAL = 120#nivel de intensidad de la imagen que será tomado como umbral\n","_,img_umbralizada = cv2.threshold(gray_img,thresh = UMBRAL,maxval = 255,type = cv2.THRESH_BINARY)\n","img_umbralizada = cv2.cvtColor(img_umbralizada,cv2.COLOR_GRAY2RGB)\n","\n","plt.figure(figsize=(6,6))\n","plt.imshow(img_umbralizada)\n","plt.show()"],"metadata":{"id":"6Kc6IJn49tg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**C. Enmascarado de imágenes**\n","\n","Usemos las imágenes binarias para enmascarar (segmentar) las zonas de interés de la imagen original:"],"metadata":{"id":"VEzmTsiVpNPO"}},{"cell_type":"code","source":["# leamos una imagen\n","img          = cv2.imread('Arroz.png')\n","img_gray     = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","ret_u,img_u  = cv2.threshold(img_gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)# se encuentra el umbral de forma automática con el método de OTSU\n","img_masked   = cv2.bitwise_and(img_gray,img_u)\n","\n","# despleguemos ambas imágenes\n","fig = plt.figure(figsize=(12,4))\n","plt.subplot(1,3,1)\n","plt.imshow(img_gray,cmap=\"gray\")\n","plt.title(\"Imagen en escala de grises\")\n","\n","plt.subplot(1,3,2)#mascara\n","plt.imshow(img_u,cmap = \"gray\")\n","plt.title(\"Imagen binarizada\")\n","\n","\n","plt.subplot(1,3,3)#imagen enmascarada\n","plt.imshow(img_masked,cmap = \"gray\")\n","plt.title(\"Imagen enmascarada\")\n","plt.show()"],"metadata":{"id":"ecOWLwX6pZit"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2. Histograma y enmascaramiento de una imagen"],"metadata":{"id":"Nd3NaGAIVnW3"}},{"cell_type":"markdown","source":["Carguemos una imagen y visualicemos su histograma"],"metadata":{"id":"S1HIqy43ilGs"}},{"cell_type":"code","source":["img      = cv2.imread('FoggyBrooklyn.jpg')\n","img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","#histograma imagen en escala de grises\n","plt.figure(figsize=(14,6))\n","plt.subplot(1,2,1)\n","plt.title(\"Imagen en escala de grises\")\n","plt.imshow(img_gray, cmap =\"gray\")\n","\n","plt.subplot(1,2,2)\n","plt.title(\"Histograma imagen en escala de grises\")\n","plt.hist(img_gray.ravel(),bins = 256, range = [0,256])\n","plt.xlabel(\"Escala de grises\"), plt.ylabel(\"Frecuencia estadística píxeles\")\n","plt.show()"],"metadata":{"id":"JyIhoTu0ir2s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Observamos que la gran parte de los valores de intensidad se encuentran en los niveles de gris de alto valor, sobre 160 aprox. Ecualicemos el histograma para mejorar el contraste de la imagen:"],"metadata":{"id":"0qddJUJ3nUHV"}},{"cell_type":"code","source":["equ = cv2.equalizeHist(img_gray)\n","\n","#Imagen e histograma después de ecualizarlo\n","plt.figure(figsize=(14,6))\n","plt.subplot(1,2,1)\n","plt.title(\"Imagen con contraste mejorado\")\n","plt.imshow(equ, cmap =\"gray\")\n","\n","plt.subplot(1,2,2)\n","plt.title(\"Histograma ecualziado\")\n","plt.hist(equ.ravel(),bins = 256, range = [0,256])\n","plt.xlabel(\"Escala de grises\"), plt.ylabel(\"Frecuencia estadística píxeles\")\n","plt.show()\n"],"metadata":{"id":"XmD9l7EWn7yX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3. Procesamiento básico sobre imágenes"],"metadata":{"id":"kHSvyz3hV1CS"}},{"cell_type":"markdown","source":["### Mejoramiento de bordes"],"metadata":{"id":"a5xY2nRLWB2l"}},{"cell_type":"code","source":["img = cv2.imread('ChestXray.jpg')#cargamos imagen\n","#Definamos kernels o filtros para mejorar la información de alta frecuencia (cambios bruscos de intensidad)\n","kernel1 = np.array([[0, -1, 0],\n","                   [-1, 5,-1],\n","                   [0, -1, 0]])\n","\n","kernel2 = np.array([[-1, -1, -1],\n","                    [-1, 9,-1],\n","                    [-1, -1, -1]])\n","image_sharp = cv2.filter2D(src=img, ddepth=-1, kernel=kernel2)\n","\n","# despleguemos ambas imágenes\n","fig = plt.figure(figsize=(12,6),dpi = 150)\n","plt.subplot(1,2,1)\n","plt.imshow(img,cmap=\"gray\")\n","plt.title(\"Imagen original\")\n","\n","plt.subplot(1,2,2)\n","plt.imshow(image_sharp,cmap = \"gray\")\n","plt.title(\"Imagen con bordes mejorados\")\n","\n","plt.show()"],"metadata":{"id":"qVuv2L4a0nEa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zfU-zcFzXwPk"},"source":["### Desenfoque Gaussiano"]},{"cell_type":"markdown","source":["En términos matemáticos, el desenfoque Gaussiano (Gaussian blurring) se realiza aplicando a la imagen original una convolución con un filtro o kernel de la forma:\n","\n","$$G(x,y)=\\frac{1}{2\\pi\\sigma^2}e^{-\\frac{x^2+y^2}{2\\sigma^2}}$$\n","\n","donde $\\sigma$ es la desviación estándar del filtro (una medida de que tan ancho es) y $x$ e $y$ son las distancias desde el origen en la dirección del eje horizontal y vertical respectivamente.\n","\n","Veamos un ejemplo, para esto carguemos una imagen:"],"metadata":{"id":"F2_JxilMWVD4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2Z_6KEPX2Yd"},"outputs":[],"source":["img = cv2.imread('KyotoJapan.jpg')\n","plt.figure(figsize=(10,10))\n","plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","plt.show()"]},{"cell_type":"markdown","source":["En OpenCV podemos implementar el desenfoque Gaussiano como sigue:"],"metadata":{"id":"VlRIa8WkXJ5M"}},{"cell_type":"code","source":["help(cv2.GaussianBlur)"],"metadata":{"id":"hjCndsDsVc4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Desenfoque Gaussiano con un kernel de 11x11\n","KERNEL_SIZE = (51,51)# el tamaño debe ser impar\n","img_blur_small = cv2.GaussianBlur(img, ksize = KERNEL_SIZE,sigmaX=0)\n","#cv2.imwrite('img-G-blur.jpg', img_blur_small)\n","plt.figure(figsize=(10,10))\n","plt.imshow(cv2.cvtColor(img_blur_small, cv2.COLOR_BGR2RGB))\n","plt.show()"],"metadata":{"id":"ieYGYNz2XJN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Algunas preguntas**\n","1. Experimentar cambiando el tamaño del kernel, qué observa en la imagen resultante?\n","2. A qué corresponde el segundo parámetro de la función, el valor de $0$ en el ejemplo anterior?\n","3. Definir y graficar el kernel Gaussiano en un mapa de intensidad usando la definición matemática dada. (utilice numpy y matplotlib)"],"metadata":{"id":"2GCggRDievh_"}},{"cell_type":"markdown","source":["##2. Ejemplo: OCR (Optical Character Recognition), Reconocimiento Óptico de Caracteres"],"metadata":{"id":"U8Sgu337t80b"}},{"cell_type":"markdown","source":["También conocido como reconocimiento de caracteres de forma simple, tiene el objetivo la digitalización de textos, el algoritmo permite identificar automáticamente los símbolos o caracteres que son parte de un determinado alfabeto, luego, al reconocerlos, podremos usar el texto para su posterior tratamiento o edición. Usaremos en este caso tanto OpenCV como la librería [Tesseract](https://en.wikipedia.org/wiki/Tesseract_(software)) para llevar a cabo la operación, experimenta con tus propios textos!!"],"metadata":{"id":"kkpha7tluX6K"}},{"cell_type":"code","source":["#Instalemos la librería\n","!sudo apt install tesseract-ocr\n","!pip install pytesseract"],"metadata":{"id":"K_Y6zJDvv1j2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Luego de instalar, reiniciar el entorno de ejecución"],"metadata":{"id":"6mLKCaoIxxwO"}},{"cell_type":"code","source":["import pytesseract #Importamos la librería\n","import matplotlib.pyplot as plt\n","import cv2, matplotlib"],"metadata":{"id":"dixJNRB5vMgB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Leamos y precesemos la imagen que contiene texto"],"metadata":{"id":"8qcDQLckwFIY"}},{"cell_type":"code","source":["img     = cv2.imread('TextoParaOCR.jpg')\n","img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.figure(figsize=(6,6))\n","plt.title(\"Imagen con texto\")\n","plt.imshow(img_rgb)\n","plt.show()"],"metadata":{"id":"Sr0pGwGLwIXv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["imprimimos el texto"],"metadata":{"id":"7THHCr0twz3H"}},{"cell_type":"code","source":["text = pytesseract.image_to_string(img_rgb)\n","print(text)"],"metadata":{"id":"lvh4kybwwzUJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preguntas:\n","1. Podremos usar la función par extraer caracteres en chino?\n","2. Cargar una imagen propia en letra imprenta o manuscrita y probar la metodología. Qué resultados obtuvo?\n","3. Experimentar con estrategias de preprocesamiento de imágenes para mejorar la exactitud de la predicción, por ej. Probar cambiando el tamaño de la imagen, umbralizando para crear una representación binaria, etc."],"metadata":{"id":"-MksIpjawI8L"}},{"cell_type":"markdown","metadata":{"id":"AS0BsxtwMBA6"},"source":["##3. Capturar una imagen desde webcam y detección de rostros\n","Capturaremos una imagen desde la webcam y ejecutaremos detección de rostro sobre esta imagen capturada. La predicción de rostros en la imagen adquirida usando el clasificador en Cascada Haar pre-entrenado y disponible en OpenCV."]},{"cell_type":"markdown","metadata":{"id":"MhuCNldGgvxT"},"source":["**Detección de Rostros:**"]},{"cell_type":"markdown","metadata":{"id":"eSEQstw4g-iI"},"source":["**Previo**: Ejecutar la siguiente función para capturar una imagen desde la webcam usando google colab (la proporciona la misma plataforma). Para código ejecutado de forma local en sus PCs, simplemente usar las funciones disponibles en OpenCV (mucho más simple, usar `VideoCapture()`)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"e_rz2QKkg8nS"},"outputs":[],"source":["#@title Captura una imagen desde webcam: take_photo()\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode , b64encode\n","\n","def take_photo(filename='photo.jpg', quality=0.8): # por defecto, la imagen capturada se llamará photo.jpg\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","\n","  # get photo data\n","  data = eval_js('takePhoto({})'.format(quality))\n","  # get OpenCV format image\n","  img = js_to_image(data)\n","  #print(gray.shape)\n","  # save image\n","  cv2.imwrite(filename, img)\n","\n","  return filename\n","\n","  #function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULOy8HJoMAno"},"outputs":[],"source":["import numpy as np\n","# Inicializamos el detector de rostros\n","face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n","\n","# realicemos una predicción sobre la imagen capturada\n","try:\n","  filename = take_photo('photo.jpg')#capturamos imagen\n","  img      = cv2.imread(filename)\n","\n","# Obtener las coordenadas de los rostros detectados con detector Cascada Haar\n","  faces = face_cascade.detectMultiScale(img)\n","# dibujar caja rectangular en rostro detectado\n","  for (x,y,w,h) in faces:\n","      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","  # Mostrar la imagen capturada y con los rectángulos que corresponda.\n","  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","  plt.show\n","except Exception as err:\n","  # except se ejecuta solo si se producen errores en try:\n","  # útil, yaque a veces al trabajar con dispositvos como webcam,\n","  # se puede desconectar o generar un error al querer conectar.\n","  print(str(err))"]},{"cell_type":"markdown","metadata":{"id":"yzXbQ1t40DZq"},"source":["## Procesamiento de un video (frame a frame)"]},{"cell_type":"markdown","metadata":{"id":"RxjGwE2kgxnz"},"source":["**Previo**: Ejecutar las siguientes funciones útiles para desplegar video en Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"f849X4RWfC17"},"outputs":[],"source":["#@title Para desplegar un video\n","import imageio\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from skimage.transform import resize\n","from IPython.display import HTML\n","\n","def display_video(video, SIZE = (6,6)):\n","    fig = plt.figure(figsize=SIZE)  #Display size specification\n","\n","    mov = []\n","    for i in range(len(video)):  #Append videos one by one to mov\n","        img = plt.imshow(video[i], animated=True)\n","        plt.axis('off')\n","        mov.append([img])\n","\n","    #Animation creation\n","    anime = animation.ArtistAnimation(fig, mov, interval=50, repeat_delay=1000)\n","\n","    plt.close()\n","    return anime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wM24MkmehhbJ"},"outputs":[],"source":["# Instalación necesario de codec\n","!pip install imageio-ffmpeg"]},{"cell_type":"markdown","metadata":{"id":"gOAXZUeEhRvg"},"source":["Subir video_sample.mp4 ([fuente](https://www.youtube.com/watch?v=njOndS-e0cw)) que se entregó con la práctica y luego ejecutar el siguiente código para desplegarlo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuT1XKPRhRCl"},"outputs":[],"source":["video = imageio.mimread('video_sample.mp4',memtest=False)  #Cargamos video\n","#video = [resize(frame, (256, 256))[..., :3] for frame in video]    #Ajuste de tamaño (si fuera necesario)\n","HTML(display_video(video, SIZE = (6,4)).to_html5_video())  #Despliegue del video en linea en HTML5"]},{"cell_type":"markdown","metadata":{"id":"Wbz_HCpLjyzZ"},"source":["Como ejemplo para guardar un video, ejecturemos un procesamiento sobre cada frame del video y lo guardaremos en un archivo `video_procesado.mp4` que podremos descargar después."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzcZ33LUjyff"},"outputs":[],"source":["cap = cv2.VideoCapture(\"video_sample.mp4\")\n","#Información del video original\n","width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","FPS    = float(cap.get(cv2.CAP_PROP_FPS))# frame rate del video original\n","\n","# Definimos el codec y creamos el objeto VideoWriter() con el nombre del video a guardar\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v') #codec\n","out    = cv2.VideoWriter('video_procesado.mp4', fourcc, FPS, (width,  height), False)# False indica que no escribiremos frames a color\n","                                                                                     # Cambiar a True si la salida serán frames a color\n","count = 0\n","# Ejecutamos el procesamiento por cada frame y guardamos frame a frame el nuevo video\n","while cap.isOpened():\n","\n","    ret, frame = cap.read()# leemos frame a frame\n","\n","    if not ret:# en caso de que no queden más frames en el video, terminamos el ciclo\n","        print(\"Can't receive frame (stream end?). Exiting ...\")\n","        break\n","\n","    # Procesamiento: flip y detección de bordes *****\n","    frame  = cv2.flip(frame, 1)#flip horizontal\n","    frame  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)# pasamos a escala de gris para facilitar procesamiento\n","    frame  = cv2.Canny(frame,threshold1=20, threshold2=150)\n","\n","    count +=1# contamos total de frames procesados\n","    # escribimos el frame procesado\n","    out.write(frame)\n","    if cv2.waitKey(1) == ord('q'):\n","        break\n","\n","print(f\"Se procesaron un total de {count} frames\")\n","# Liberamos objetos de memoria cuando se termine el loop\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"--RIxSB-rrPf"},"source":["Despleguemos el video procesado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GZnvlGOrtWf"},"outputs":[],"source":["video = imageio.mimread('video_procesado.mp4',memtest=False)  #Loading video\n","HTML(display_video(video, SIZE = (6,4)).to_html5_video())  #Inline video display in HTML5"]},{"cell_type":"markdown","metadata":{"id":"JAmhTG08CQ0W"},"source":["## Detección de rostros, ejemplo script con webcam"]},{"cell_type":"markdown","metadata":{"id":"480k3dpeZ_Qk"},"source":["Abrir con algún IDE que soporte Python, script `Detecta_Rostros_Webcam.py`. Requiere que instalen OpenCV. En Anaconda, instalarlo desde el Anaconda Prompt con el siguiente comando o directo en el IDE: `pip install opencv-python`"]},{"cell_type":"markdown","metadata":{"id":"Cw5Bv7vWMat2"},"source":["##**Ejercicios**"]},{"cell_type":"markdown","source":["1. Implementar el detector en cascada pero usandolo para detectar autos, en internet existen modelos pre-entrenados para openCV que implementan esta solución. Usar el video adjunto `Traffic.mp4` o uno propio para probarlo."],"metadata":{"id":"-A-1SubHz3bX"}},{"cell_type":"markdown","metadata":{"id":"c8Xw0i9xMc0j"},"source":["2. Cargar y procesar un video. Guardar un video que contenga el video original, al lado superior derecho la componente R de los frames, abajo a la izquierda, el canal G, y abajo a la derecha el canal B.  "]},{"cell_type":"markdown","source":["3. En el ejemplo visto de detección de rostros usando webcam (archivo `Detecta_Rostros_Webcam.py`), al llamar al detector de rostros para realizar predicciones `face_cascade.detectMultiScale(gray, 1.1, 4)`, ¿qué representan el primer y segundo parámetros con valores `1.1` y `4` respectivamente?"],"metadata":{"id":"zj5_-hMznqOA"}},{"cell_type":"markdown","metadata":{"id":"1R4Nn4XBOZha"},"source":["## Referencias"]},{"cell_type":"markdown","metadata":{"id":"cbShiR4UOb06"},"source":["**1.**  Detectores en Cascada con OpenCV: [enlace](https://pyimagesearch.com/2021/04/12/opencv-haar-cascades/)."]},{"cell_type":"markdown","source":["OTROS EJEMPLOS\n"],"metadata":{"id":"q0f1uhChIXOr"}},{"cell_type":"markdown","source":["- OpenCV OCR + Tesseract: https://pyimagesearch.com/2018/09/17/opencv-ocr-and-text-recognition-with-tesseract/\n","- OCR con Keras-OCR: https://www.analyticsvidhya.com/blog/2022/09/extract-text-from-images-quickly-using-keras-ocr-pipeline/"],"metadata":{"id":"GTKTZfKfIYkx"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjrA7NGJKJ6a6djkC6aTkg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}