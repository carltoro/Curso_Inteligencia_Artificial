{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNK5UNHPc1YAXljHg6tI+0t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#**Práctica 8: Transferencia de aprendizaje (Transfer Learning)**\n","\n","Curso: Inteligencia Artificial para Ingenieros\n","\n","Prof. Carlos Toro N. (carlos.toro.ing@gmail.com)\n","\n","2022"],"metadata":{"id":"3PU8MnGgYp50"}},{"cell_type":"markdown","source":["## **INTRODUCCIÓN**"],"metadata":{"id":"OCjkQvejKvly"}},{"cell_type":"markdown","source":["En la práctica, entrenar una red neuronal convolucional (ConvNet) diseñándola desde cero es raro, dado que pocas veces se cuenta con conjuntos de datos lo suficientemente grandes, es común pre-entrenar una ConvNet en un dataset muy grande (por ej. ImageNet, que contiene millones de imágenes con 1000 categorías), y luego usar esa ConvNet para la tarea de interés. Como se muestra en la siguiente figura ([fuente](https://www.mdpi.com/1424-8220/20/23/6713/htm) ), podemos aprovechar de dos formas principales estos modelos pre-entrenados (**a**), usándolos como un extractor de características (**b**) o como un modelo base al cual se le pueden ajustar algunas capas para mejorar los resultados de nuestro problema (**c**).\n","\n","<center>\n","    <img width=\"60%\" src=\"https://www.mdpi.com/sensors/sensors-20-06713/article_deploy/html/images/sensors-20-06713-g009.png\">\n","</center>\n","\n","\n","En esta práctica veremos la estrategia de transferencia de aprendizaje (transfer learning) donde usaremos un modelo pre-entrenado en ImageNet para usarlo como base en un problema de clasificación binaria de avellanas en buen o mal estado en una linea de producción, las imagenes fueron descargadas y adaptadas desde [aquí](https://www.mvtec.com/company/research/datasets/mvtec-ad/)."],"metadata":{"id":"yfRR8Zn1145c"}},{"cell_type":"markdown","source":["## **Ejemplo Tranfer Learning (Transferencia de Aprendizaje)**"],"metadata":{"id":"mlrZ3Ld9MoJj"}},{"cell_type":"markdown","source":["**Importaciones necesarias**"],"metadata":{"id":"TCxk_cE9SCbl"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"1Xg9BFDrSEE0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cargamos los datos** desde dropbox (respáldenlos para futuros experimentos, puede que los borre eventualmente). Otra forma sería dejar alojados los datos en Google Drive y montar el disco para leerlos directo desde su drive, yaque la memoria de disco de Colab es limitada ( en este ejemplo es suficiente).\n"],"metadata":{"id":"D34sD7bSElmJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2PCs0EuYgrL"},"outputs":[],"source":["!wget https://www.dropbox.com/s/av931fekqljr0jj/hazelnut_data.zip\n","# descomprimimos el archivo con las imágenes\n","!unzip -q hazelnut_data.zip\n","!rm hazelnut_data.zip # eliminamos el archivo comprimido para liberar memoria"]},{"cell_type":"markdown","source":["**Preparamos los datos usando los generadores de Tensorflow para estructurarlos en un formato de dataset** que no implique cargar todos los archivos en la ram a la vez, si no que se llamen cuando se necesiten durante el entrenamiento. En este caso el tamaño de la imagen que definiremos para los datasets tiene relación con el que aceptará el modelo pre-entrenado que cargaremos, en esta práctica el MobileNetV2, disponible [aquí](https://www.tensorflow.org/api_docs/python/tf/keras/applications)."],"metadata":{"id":"H79tXlC5wY6S"}},{"cell_type":"code","source":["# Parámetros que aceptará nuestro modelo posterior y para el entrenamiento (a gusto del consumidor!)\n","IMG_WIDTH, IMG_HEIGHT = 224, 224 # Para llevar las imagenes al tamaño del modelo pre-entrenado que cargaremos, cambiarlo adecuadamente según sea la necesidad\n","BATCH_SIZE            = 32\n","SEED_TRAIN_VAL        = 1 # semilla generadora para los datasets, debe ser igual para todos\n","\n","# Una forma con una función que no eliminarán, la anterior ImageDataGenerator la sacarán en versiones futuras\n","path_base = '/content/DATA' # directorio actual donde están las carpetas con imágenes\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","                                                        path_base,\n","                                                        validation_split = 0.3,\n","                                                        subset           = \"training\",\n","                                                        seed             = SEED_TRAIN_VAL,\n","                                                        image_size       = (IMG_HEIGHT, IMG_WIDTH),\n","                                                        batch_size       = BATCH_SIZE)\n","\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","                                                     path_base,\n","                                                     validation_split= 0.3,\n","                                                     subset          = \"validation\",\n","                                                     seed            = SEED_TRAIN_VAL,\n","                                                     image_size      = (IMG_HEIGHT, IMG_WIDTH),\n","                                                     batch_size      = BATCH_SIZE)\n","\n","\n","# creamos el conjunto de test, dejamos 20% para test y 10% para validación, del 30% original\n","val_batches = tf.data.experimental.cardinality(val_ds)\n","test_ds     = val_ds.take((2*val_batches) // 3)\n","val_ds      = val_ds.skip((2*val_batches) // 3)\n","\n","#nombre de las clases\n","class_names = train_ds.class_names\n","print(class_names)"],"metadata":{"id":"FTuh_BQ9wdVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para un mayor control, estructurar los datos en una carpeta de train, otra de test, y otra de validación, que contengan a su vez sub-carpetas con los archivos de imágenes correspondientes."],"metadata":{"id":"JZbzgyQOSy6I"}},{"cell_type":"code","source":["# visualización de algunas de las imágenes\n","plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype(\"uint8\"))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")"],"metadata":{"id":"kM-oDiOlJGhf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Configuración de los conjuntos de dato para un mejor rendimiento al llamar las imágenes desde el disco duro."],"metadata":{"id":"BNaOt-MdS9mi"}},{"cell_type":"code","source":["AUTOTUNE = tf.data.AUTOTUNE\n","train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","test_ds  = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"],"metadata":{"id":"boMPpal0ULgB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generamos capas de pre-procesamiento y así aumentar el dataset original con algunas operaciones:"],"metadata":{"id":"NY3qASVtUHhD"}},{"cell_type":"code","source":["data_augmentation = keras.Sequential([  layers.Rescaling(1./255, offset = -1), #para dejar los valores de intensidad entre [-1,1], lo que espera nuestro modelo mobilnetv2 que usaremos\n","                                                                               #verificar siempre según el modelo base que se vaya a usar cual es el rango esperado.\n","                                        layers.RandomFlip(\"horizontal_and_vertical\"),\n","                                        layers.RandomRotation(0.2),\n","                                        layers.RandomZoom(.1, .1)\n","                                     ])"],"metadata":{"id":"mcWTfFkWTCLV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Usaremos MobileNetV2, si se usa otro, tener cuidado con las dimensiones de la imagen de entrada. Este modelo actuará como un extractor de características de las imagenes para luego pasarselas al clasificador"],"metadata":{"id":"KTgvh5QFB0J2"}},{"cell_type":"code","source":["IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT,3)# agregamos la dimensión 3 yaque el modelo acepta imágenes a color\n","# Cargamos un modelo pre-entrenado en el dataset de ImageNet\n","modelo_base = keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n","                                             include_top=False,\n","                                             weights='imagenet')\n","\n","# es importante congelar la etapa convolucional para que no se entrene nuevamente\n","modelo_base.trainable = False"],"metadata":{"id":"VSMZSlssYCGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_EPOCAS  = 200 #máximo número de épocas\n","\n","# Definición del modelo completo\n","modelo    = keras.Sequential([ layers.Input(shape=IMG_SHAPE),\n","                               data_augmentation,\n","                               modelo_base,\n","                               layers.GlobalAveragePooling2D(),\n","                               layers.Dropout(.2),\n","                               layers.Dense(1,activation = 'sigmoid')\n","                             ])\n","\n","\n","# Configuración de estrategia de detención temprana\n","PATIENCE = 20\n","early    = tf.keras.callbacks.EarlyStopping( patience=PATIENCE, monitor=\"val_loss\", restore_best_weights=True )\n","\n","# Configuración del entrenamiento\n","base_learning_rate = 0.01#tasa de aprendizaje base, se puede experimentar con este valor de partida, pero al usar transfer learning, usar valores bajos\n","modelo.compile(optimizer= keras.optimizers.Adam(lr=base_learning_rate),\n","               loss     = keras.losses.BinaryCrossentropy(from_logits=True),\n","               metrics  = ['accuracy'])\n","\n","modelo.summary()"],"metadata":{"id":"yv38vp4gU_56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Entrenamiento**"],"metadata":{"id":"vrIn_HlSb6NL"}},{"cell_type":"code","source":["# Entrenamiento\n","history = modelo.fit(train_ds, validation_data=val_ds, epochs=N_EPOCAS,callbacks = early)"],"metadata":{"id":"NDshZs4Zb9Kk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Algunas gráficas del entrenamiento:"],"metadata":{"id":"mxgf7Mq6d7UU"}},{"cell_type":"code","source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Accuracy en Entrenamiento')\n","plt.plot(val_acc, label='Accuracy en Validación')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Accuracy en Entrenamiento y Validación')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Pérdida en Entrenamiento')\n","plt.plot(val_loss, label='Pérdida en Validación')\n","plt.legend(loc='upper right')\n","plt.ylabel('Entropía Cruzada')\n","plt.ylim([0,1.0])\n","plt.title('Pérdida en Entrenamiento y Validación')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"id":"6N5_cC4qd_zT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Resultados finales en training\n","print('Loss final en training: ',history.history['loss'][-1])\n","print('Accuracy final en training: ',history.history['accuracy'][-1])"],"metadata":{"id":"jUwJeLUBWGhm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Resultados en conjunto de prueba**"],"metadata":{"id":"6jjg8cuDd9lp"}},{"cell_type":"code","source":["# Evaluamos en conjunto de prueba\n","score = modelo.evaluate(test_ds, verbose=0)\n","\n","print('Loss en dataset de prueba: ',score[0])\n","print('Accuracy en dataset de prueba: ',score[1])"],"metadata":{"id":"Shv3fTpBVvYx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Matriz de confusión en el conjunto de prueba**"],"metadata":{"id":"eCkwgOq3sBoe"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n","import seaborn as sns\n","\n","y_pred = modelo.predict(test_ds, verbose = 0)#valores predichos con el modelo\n","y_pred = (y_pred>0.5).astype('int32') #binarización de la salida, yaque la sigmoidal entrega valores entre 0 y 1 a la salida de la red.\n","\n","#etiquetas reales conjunto de test\n","y_test = np.concatenate([y for x, y in test_ds], axis=0)\n","\n","\n","cm   = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=class_names)\n","disp.plot(cmap=plt.cm.Blues)\n","plt.show()"],"metadata":{"id":"XvN2Vd4mtvOQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Resumiendo**\n","En esta práctica vimos:\n","- Cómo crear un dataset a partir de imágenes propias\n","- Reutilizar un modelo pre-entrenado en un dataset grande\n","- Implementar la estrategia de transferencia de aprendizaje\n","- Evaluar el desempeño de un modelo\n",""],"metadata":{"id":"FazT9toLUU4N"}},{"cell_type":"markdown","source":["## **Ejercicios Extra**"],"metadata":{"id":"f44mQCGuNbcv"}},{"cell_type":"markdown","source":["**E.1.** Repetir el proceso anterior pero con otros modelos pre-entrenados disponibles en tensorflow, comparar los resultados y quedarse con el mejor de acuerdo a los criterios que estime convenientes."],"metadata":{"id":"klKPBWNbNe3a"}},{"cell_type":"code","source":["#Código aquí"],"metadata":{"id":"j6kO9AlANtqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**E.2.** Repetir **E.1.** pero esta vez para **resolver un problema de clasificación multiclase** con imágenes r**ecolectadas por ustedes mismos**. Estructurar adecuadamente las carpetas con los datos, generar los datasets, definir los modelos, evaluar el desempeño del modelo y guardar el modelo para futuros usos."],"metadata":{"id":"iq-I2f8AN8Ub"}},{"cell_type":"code","source":["#Código aquí"],"metadata":{"id":"w_6NyV2SOVbI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**E.3.** Vimos que colab es una buena herramienta para entrenar modelos de deep learning yaque nos facilita el uso de GPUs de los servidores de Google. Generar un script .py de forma local, usando por ejemplo Spyder, para realizar inferencias en nuevas imágenes obtenidas en el punto **E.2.** y que utilice el modelo entrenado en colab. Los desafíos acá serán configurar adecuadamente un ambiente de python para trabajar con las librerías actualizadas o que requieran instalarse, con Anaconda esto se puede realizar de forma sencilla."],"metadata":{"id":"_aX9o3jrOYZC"}},{"cell_type":"markdown","source":["##**Referencias**"],"metadata":{"id":"TEhAehs_YhaO"}},{"cell_type":"markdown","source":["* Transfer Learning, CS231n: Convolutional Neural Networks for Visual Recognition, Curso Standford, 2022, [online](http://cs231n.stanford.edu/)\n","* Ejemplo de clasificación multiclase usando Transfer Learning: [video](https://www.youtube.com/watch?v=EkAg51oIvQI)\n"],"metadata":{"id":"z1i_HgcW0Z64"}}]}